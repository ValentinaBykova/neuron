{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTrPrPlWDOT4b5GROY3yo4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinaBykova/neuron/blob/master/lab8/Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторна робота №8\n",
        "## студентки КН-31, Бикової Валентини\n",
        "## Тема: Творче завдання"
      ],
      "metadata": {
        "id": "21SUGgVhLLDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Нейронна мережа Елмана"
      ],
      "metadata": {
        "id": "yBsuzv_uasS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S79IB3_FLI5Q"
      },
      "outputs": [],
      "source": [
        "from typing import Union, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Sigmoid activation function\n",
        "\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def softmax(x: Union[np.ndarray, List]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Softmax activation function\n",
        "\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def softmax_der(x: Union[np.ndarray, List]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Derivative of softmax activation function\n",
        "\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "    res = softmax(x) * (1 - softmax(x))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def sigmoid_der(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Derivative of sigmoid activation function\n",
        "\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "\n",
        "def log(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Log activation function (natural algorithm)\n",
        "\n",
        "    :url: http://jmlda.org/papers/doc/2011/no1/Rudoy2011Selection.pdf#page=12\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    res = np.log(x + np.sqrt(x**2 + 1))\n",
        "    res[x > 74.2] = 5\n",
        "    res[x < -74.2] = -5\n",
        "    res = res / 5\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def log_der(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Derivative of log activation function (natural algorithm)\n",
        "\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    res = 1 / (np.sqrt(x**2 + 1))\n",
        "    res[x > 74.2] = 0\n",
        "    res[x < -74.2] = 0\n",
        "    res = res / 5\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def linear(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Linear activation function (natural algorithm)\n",
        "\n",
        "    :url: http://jmlda.org/papers/doc/2011/no1/Rudoy2011Selection.pdf#page=12\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def linear_der(x: np.ndarray) -> Union[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Derivative of log activation function (natural algorithm)\n",
        "\n",
        "    :param x: input matrix\n",
        "    :return: resulted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    return 1\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y_pred: Union[np.ndarray, List],\n",
        "                       y_true: Union[np.ndarray, List]) -> List[Union[float, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Cross entropy loss.\n",
        "    Use it after sigmoid function!\n",
        "\n",
        "    :param y_pred: values predicted by NN (with softmax on top of it)\n",
        "    :param y_true: true value\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "    y_pred = np.array(y_pred, dtype=np.float16)\n",
        "\n",
        "    y_true = np.array(y_true, dtype=np.float16)\n",
        "    y_true_argmax = y_true.argmax(axis=0)\n",
        "\n",
        "    y_pred[y_true_argmax] = np.clip(y_pred[y_true_argmax], a_min=0.0001, a_max=None)\n",
        "\n",
        "    log_likelihood = - np.log(y_pred[y_true_argmax])\n",
        "    loss = np.sum(log_likelihood)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cross_entropy_loss_der(y_pred: Union[np.ndarray, List],\n",
        "                           y_true: Union[np.ndarray, List]) -> Union[List[float], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Cross entropy loss derivative.\n",
        "    Use it after sigmoid function!\n",
        "\n",
        "    :param y_pred: values predicted by NN (with softmax, sigmoid on top of it)\n",
        "    :param y_true: true value\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "    y_pred = np.array(y_pred, dtype=np.float16)\n",
        "    grad = linear(x=y_pred)\n",
        "\n",
        "    y_true = np.array(y_true, dtype=np.float16)\n",
        "    y_true_argmax = y_true.argmax(axis=0)\n",
        "\n",
        "    grad[y_true_argmax] = np.clip(grad[y_true_argmax], a_min=0.0001, a_max=None)\n",
        "    grad[y_true_argmax] -= 1\n",
        "\n",
        "    step = - grad\n",
        "\n",
        "    return step\n",
        "\n",
        "\n",
        "def mse_loss(y_pred: Union[np.ndarray, List],\n",
        "             y_true: Union[np.ndarray, List]) -> List[Union[float, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Mean squared error loss\n",
        "\n",
        "    :param y_pred: values predicted by NN (WITHOUT softmax, sigmoid on top of it)\n",
        "    :param y_true: true value\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "\n",
        "    mse_value = ((y_pred - y_true) ** 2).mean()\n",
        "\n",
        "    return mse_value\n",
        "\n",
        "\n",
        "def mse_loss_der(y_pred: Union[np.ndarray, List],\n",
        "                 y_true: Union[np.ndarray, List]) -> List[Union[float, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Mean squared error loss derivative\n",
        "\n",
        "    :param y_pred: values predicted by NN (WITHOUT softmax, sigmoid on top of it)\n",
        "    :param y_true: true value\n",
        "    :return: loss value\n",
        "    \"\"\"\n",
        "\n",
        "    mse_der_value = - 2 * (y_pred - y_true)\n",
        "\n",
        "    return mse_der_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Jordan:\n",
        "    \"\"\"Jordan network\"\"\"\n",
        "\n",
        "    def __init__(self, lr: float,\n",
        "                 momentum: float,\n",
        "                 shape: List[int], make_zero_context: bool = False):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.make_zero_context = make_zero_context\n",
        "\n",
        "        self.shape = shape\n",
        "\n",
        "        self.n_layers = len(shape)\n",
        "\n",
        "        self.layers = self.__init_layers__()\n",
        "        self.weights = self.__init_weights__()\n",
        "\n",
        "        self.dw = [0] * len(self.weights)\n",
        "\n",
        "    def __init_layers__(self) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Initialize layers of NN\n",
        "\n",
        "        :return: list of initialized layers\n",
        "        \"\"\"\n",
        "\n",
        "        layers = list()\n",
        "\n",
        "        layers.append(np.ones(self.shape[0] + self.shape[-1] + 1))\n",
        "\n",
        "        for i in range(1, self.n_layers):\n",
        "            layers.append(np.ones(self.shape[i]))\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def __init_weights__(self) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Neural networks he initialization\n",
        "\n",
        "        :url: https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
        "        :return: list of weights\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.layers) == 0:\n",
        "            raise ValueError('Before weight initialization, initialize layers!')\n",
        "\n",
        "        weights = list()\n",
        "\n",
        "        for i in range(self.n_layers - 1):\n",
        "            curr_weights = np.random.randn(self.layers[i].size,\n",
        "                                           self.layers[i + 1].size) * np.sqrt(2 / self.layers[i].size)\n",
        "\n",
        "            weights.append(curr_weights)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def propagate_forward(self, x: Union[np.ndarray, List]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Propagate data in network forward. Forward pass\n",
        "\n",
        "        :param x: data to propagate\n",
        "        :return: result of neural network\n",
        "        \"\"\"\n",
        "\n",
        "        self.layers[0][0: self.shape[0]] = x\n",
        "\n",
        "        if self.make_zero_context:\n",
        "            self.layers[0][self.shape[0]: -1] = np.zeros_like(self.layers[-1])\n",
        "        else:\n",
        "            self.layers[0][self.shape[0]: -1] = self.layers[-1]\n",
        "\n",
        "        for i in range(1, len(self.shape) - 1):\n",
        "            self.layers[i][...] = linear(\n",
        "                np.dot(self.layers[i - 1], self.weights[i - 1])\n",
        "            )\n",
        "\n",
        "        if len(self.shape) - 2 >= 0:\n",
        "            last_idx = len(self.shape) - 1\n",
        "\n",
        "            self.layers[last_idx][...] = linear(\n",
        "                np.dot(self.layers[last_idx - 1], self.weights[last_idx - 1])\n",
        "            )\n",
        "\n",
        "        return self.layers[-1]\n",
        "\n",
        "    def propagate_backward(self, target) -> float:\n",
        "        \"\"\"\n",
        "        Performs backpropagation on neural network.\n",
        "\n",
        "        :param target: desired result\n",
        "        :return: error of the network\n",
        "        \"\"\"\n",
        "\n",
        "        deltas = list()\n",
        "\n",
        "        loss_number = mse_loss(y_pred=self.layers[-1],\n",
        "                               y_true=target)\n",
        "        last_layer_delta = mse_loss_der(y_pred=self.layers[-1],\n",
        "                                        y_true=target)\n",
        "\n",
        "        deltas.append(last_layer_delta)\n",
        "\n",
        "        for i in range(len(self.shape) - 2, 0, -1):\n",
        "            curr_delta = np.dot(deltas[0],\n",
        "                                self.weights[i].T * linear_der(self.layers[i]))\n",
        "\n",
        "            deltas.insert(0, curr_delta)\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            layer = np.atleast_2d(self.layers[i])\n",
        "            curr_delta = np.atleast_2d(deltas[i])\n",
        "\n",
        "            curr_dw = np.dot(layer.T, curr_delta)\n",
        "\n",
        "            self.weights[i] += self.lr * curr_dw + self.lr * self.momentum * self.dw[i]\n",
        "\n",
        "            self.dw[i] = curr_dw\n",
        "\n",
        "        return loss_number\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    network = Jordan(lr=0.01, momentum=0.1, shape=[10, 15, 15, 1])\n",
        "\n",
        "    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    result = network.propagate_forward(x=x)\n",
        "    error = network.propagate_backward(target=1, )\n",
        "\n",
        "    print(f'Result: {result}')\n",
        "    print(f'Error: {error}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ca8XzGa0N_",
        "outputId": "a6c1df0d-07ef-4a26-a316-003977439908"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: [11.51516912]\n",
            "Error: 110.56878157022702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Висновики\n",
        "#### Під час лабораторної роботи я вивчила новий для себе вид нейронних мереж - мережа Елмана та розглянула її застосування."
      ],
      "metadata": {
        "id": "sW-AEkHJbfNA"
      }
    }
  ]
}